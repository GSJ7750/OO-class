{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "8\n",
      "12\n",
      "17\n",
      "23\n",
      "25\n",
      "28\n",
      "32\n",
      "36\n",
      "39\n",
      "42\n",
      "Epoch  100 | incorrect  98.4% | mean squ error  2281.6\n",
      "Epoch  200 | incorrect  98.4% | mean squ error  667.6\n",
      "Epoch  300 | incorrect  1.6% | mean squ error  164.8\n",
      "Epoch  400 | incorrect  1.6% | mean squ error  53.5\n",
      "Epoch  500 | incorrect  1.6% | mean squ error  36.4\n",
      "Epoch  600 | incorrect  1.6% | mean squ error  34.6\n",
      "Epoch  700 | incorrect  1.6% | mean squ error  34.5\n",
      "Epoch  800 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch  900 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1000 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1100 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1200 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1300 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1400 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1500 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1600 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1700 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1800 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 1900 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2000 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2100 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2200 | incorrect  1.6% | mean squ error  34.4\n",
      "Epoch 2300 | incorrect  1.6% | mean squ error  34.4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-68a1b6f6cb4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;31m##################################################################### backward lstm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf1.8\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://www.bbc.com/korean/news-46562902'\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "mr = soup.find(class_=\"story-body__introduction\")\n",
    "text = mr.get_text()\n",
    "split_text = list(text)\n",
    "\n",
    "spacing = []\n",
    "for i in range(len(split_text)):\n",
    "    if split_text[i] == ' ':\n",
    "        spacing.append(([split_text[i-1], split_text[i+1]]))\n",
    "        \n",
    "split1 = split_text\n",
    "df = pd.get_dummies(split1)\n",
    "split_text = list(filter((' ').__ne__, split_text))\n",
    "\n",
    "split2 = split_text\n",
    "df2 = pd.get_dummies(split2)\n",
    "\n",
    "xdata = df2.values\n",
    "y = np.ones([xdata.shape[0], 1])\n",
    "\n",
    "for k in range(len(spacing)):\n",
    "    for l in range(len(split2)):\n",
    "        if split2[l] == spacing[k][0]:\n",
    "            if split2[l+1] == spacing[k][1]:\n",
    "                print(\"spacing : \", l)\n",
    "                y[l][0] = y[l][0]-1\n",
    "                \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "sess = tf.Session()\n",
    "\n",
    "x = xdata\n",
    "\n",
    "\n",
    "y_ = np.zeros([xdata.shape[0], xdata.shape[0]-1])\n",
    "\n",
    "y = np.concatenate((y, y_),axis = 1)\n",
    "\n",
    "\n",
    "c_ = tf.zeros([xdata.shape[0],xdata.shape[0]])\n",
    "h_ = tf.zeros([xdata.shape[0],xdata.shape[0]])\n",
    "\n",
    "\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, xdata.shape[1]])\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None, xdata.shape[0]])\n",
    "W = tf.Variable(tf.random_normal([xdata.shape[0], 1]))*0.5\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "\n",
    "#he\n",
    "#el\n",
    "#ll\n",
    "#lo\n",
    "\n",
    "seq_len = len(x) #len(x)cell 갯수, 인풋이 몇 덩어리인지\n",
    "num_units = len(sess.run(c_))  #len(sess.run((c_)))# hiddenlayer\n",
    "\n",
    "\n",
    "class lstm:\n",
    "    def build(c, h):\n",
    "        args = tf.concat((X,h), axis=1)\n",
    "#        print(args)\n",
    "\n",
    "        out_size = 4 * num_units\n",
    "        proj_size = args.shape[-1]\n",
    "#        print(out_size)\n",
    "#        print(proj_size)\n",
    "\n",
    "        weights = tf.ones([proj_size, out_size]) * 0.5\n",
    "#        print(weights)\n",
    "\n",
    "\n",
    "        out = tf.matmul(args, weights)\n",
    "#        print(out)\n",
    "\n",
    "        bias = tf.ones([out_size]) * 0.5\n",
    "#        print(bias)\n",
    "\n",
    "        concat = out + bias\n",
    "#        print(concat)\n",
    "\n",
    "        i, j, f, o = tf.split(concat, 4, 1)\n",
    "#        print(i)\n",
    "#        print(j)\n",
    "#        print(f)\n",
    "#        print(o)\n",
    "\n",
    "        g = tf.tanh(j)\n",
    "#        print(g)\n",
    "\n",
    "        def sigmoid_array(x):\n",
    "            return 1 / (1 + tf.exp(-x))\n",
    "\n",
    "        forget_bias = 1.0\n",
    "\n",
    "        sigmoid_f = sigmoid_array(f + forget_bias)\n",
    "#        print(sigmoid_f)\n",
    "\n",
    "        sigmoid_array(i) * g\n",
    "\n",
    "        new_c = c * sigmoid_f + sigmoid_array(i) * g\n",
    "#        print(new_c)\n",
    "\n",
    "        new_h = tf.tanh(new_c) * sigmoid_array(o)\n",
    "#        print(new_h)\n",
    "\n",
    "#        print('\\n new_h:',new_h)\n",
    "#        print('\\n new_c',new_c)\n",
    "\n",
    "#        print(res[1].h)\n",
    "#        print(res[1].c)\n",
    "\n",
    "        return new_c, new_h\n",
    "\n",
    "bx = x[::-1]\n",
    "\n",
    "by = y[::-1]\n",
    "\n",
    "\n",
    "\n",
    "bc_ = tf.zeros([xdata.shape[0],xdata.shape[0]])\n",
    "bh_ = tf.zeros([xdata.shape[0],xdata.shape[0]])\n",
    "\n",
    "\n",
    "bX = tf.placeholder(dtype=tf.float32, shape=[None, xdata.shape[1]])\n",
    "bY = tf.placeholder(dtype=tf.float32, shape=[None, xdata.shape[0]])\n",
    "bW = tf.Variable(tf.random_normal([xdata.shape[0], 1]))*0.5\n",
    "bb = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "class blstm:\n",
    "    def build(c, h):\n",
    "        args = tf.concat((X,h), axis=1)\n",
    "#        print(args)\n",
    "\n",
    "        out_size = 4 * num_units\n",
    "        proj_size = args.shape[-1]\n",
    "#        print(out_size)\n",
    "#        print(proj_size)\n",
    "\n",
    "        weights = tf.ones([proj_size, out_size]) * 0.5\n",
    "#        print(weights)\n",
    "\n",
    "\n",
    "        out = tf.matmul(args, weights)\n",
    "#        print(out)\n",
    "\n",
    "        bias = tf.ones([out_size]) * 0.5\n",
    "#        print(bias)\n",
    "\n",
    "        concat = out + bias\n",
    "#        print(concat)\n",
    "\n",
    "        i, j, f, o = tf.split(concat, 4, 1)\n",
    "#        print(i)\n",
    "#        print(j)\n",
    "#        print(f)\n",
    "#        print(o)\n",
    "\n",
    "        g = tf.tanh(j)\n",
    "#        print(g)\n",
    "\n",
    "        def sigmoid_array(x):\n",
    "            return 1 / (1 + tf.exp(-x))\n",
    "\n",
    "        forget_bias = 1.0\n",
    "\n",
    "        sigmoid_f = sigmoid_array(f + forget_bias)\n",
    "#        print(sigmoid_f)\n",
    "\n",
    "        sigmoid_array(i) * g\n",
    "\n",
    "        new_bc = c * sigmoid_f + sigmoid_array(i) * g\n",
    "#        print(new_c)\n",
    "\n",
    "        new_bh = tf.tanh(new_bc) * sigmoid_array(o)\n",
    "#        print(new_h)\n",
    "\n",
    "#        print('\\n new_h:',new_h)\n",
    "#        print('\\n new_c',new_c)\n",
    "\n",
    "#        print(res[1].h)\n",
    "#        print(res[1].c)\n",
    "\n",
    "        return new_bc, new_bh\n",
    "\n",
    "##################################################################### Forward lstm\n",
    "\n",
    "ta_c = tf.TensorArray(size=seq_len, dtype=tf.float32)\n",
    "ta_h = tf.TensorArray(size=seq_len, dtype=tf.float32)\n",
    "\n",
    "def body(last_state, last_output, step, ta_c, ta_h):\n",
    "    \n",
    "    output = lstm.build(last_state, last_output)[0]\n",
    "    state = lstm.build(last_state, last_output)[1]\n",
    "    ta_c = ta_c.write(step, state)\n",
    "    ta_h = ta_h.write(step, output)\n",
    "    return state, output, tf.add(step, 1), ta_c, ta_h\n",
    "    \n",
    "\n",
    "timesteps = seq_len\n",
    "steps = lambda a, b, step, c, d: tf.less(step, timesteps)\n",
    "lstm_output, lstm_state, step, ta_c, ta_h = tf.while_loop(steps, body, (c_, h_, 0, ta_c, ta_h), parallel_iterations=20)\n",
    "\n",
    "output = lstm_output\n",
    "logits = tf.matmul(output, W) + b\n",
    "\n",
    "with tf.name_scope('mean_square_error'):\n",
    "    mean_square_error = tf.reduce_sum(tf.square(tf.subtract(Y, tf.unstack(logits, axis = 1))))\n",
    "tf.summary.scalar('mean_square_error', mean_square_error)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.0003)\n",
    "minimize = optimizer.minimize(mean_square_error)\n",
    "\n",
    "with tf.name_scope('error'):\n",
    "    with tf.name_scope('mistakes'):\n",
    "        mistakes = tf.not_equal(Y, tf.round(tf.unstack(logits, axis = 1)))\n",
    "    with tf.name_scope('error'):\n",
    "        error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "tf.summary.scalar('error', error)\n",
    "\n",
    "sess = tf.Session()\n",
    "merged = tf.summary.merge_all()\n",
    "date = str(datetime.datetime.now())\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "epoch = 3000\n",
    "for i in range(epoch):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        summary, incorrect, mean_squ_err = sess.run([merged, error, mean_square_error], {X:x, Y:y})\n",
    "        \n",
    "        print('Epoch {:4d} | incorrect {: 3.1f}% | mean squ error {: 3.1f}'.format(i + 1, incorrect * 100, mean_squ_err))\n",
    "    else:\n",
    "        summary, acc = sess.run([merged, error], {X:x, Y:y})\n",
    "\n",
    "\n",
    "    sess.run(minimize,{X:x, Y:y})\n",
    "    \n",
    "##################################################################### backward lstm\n",
    "    \n",
    "bta_c = tf.TensorArray(size=seq_len, dtype=tf.float32)\n",
    "bta_h = tf.TensorArray(size=seq_len, dtype=tf.float32)\n",
    "\n",
    "def bbody(last_state, last_output, step, bta_c, bta_h):\n",
    "    \n",
    "    boutput = blstm.build(last_state, last_output)[0]\n",
    "    bstate = blstm.build(last_state, last_output)[1]\n",
    "    bta_c = bta_c.write(step, bstate)\n",
    "    bta_h = bta_h.write(step, boutput)\n",
    "    return bstate, boutput, tf.add(step, 1), bta_c, bta_h\n",
    "    \n",
    "\n",
    "timesteps = seq_len\n",
    "\n",
    "\n",
    "steps = lambda a, b, step, c, d: tf.less(step, timesteps)\n",
    "\n",
    "blstm_output, blstm_state, step, bta_c, bta_h = tf.while_loop(steps, bbody, (bc_, bh_, 0, bta_c, bta_h), parallel_iterations=20)\n",
    "\n",
    "boutput = blstm_output\n",
    "blogits = tf.matmul(boutput, W) + b\n",
    "\n",
    "with tf.name_scope('mean_square_error'):\n",
    "    mean_square_error = tf.reduce_sum(tf.square(tf.subtract(Y, tf.unstack(logits, axis = 1))))\n",
    "tf.summary.scalar('mean_square_error', mean_square_error)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(0.0003)\n",
    "minimize = optimizer.minimize(mean_square_error)\n",
    "\n",
    "with tf.name_scope('error'):\n",
    "    with tf.name_scope('mistakes'):\n",
    "        mistakes = tf.not_equal(Y, tf.round(tf.unstack(logits, axis = 1)))\n",
    "    with tf.name_scope('error'):\n",
    "        error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "tf.summary.scalar('error', error)\n",
    "\n",
    "sess = tf.Session()\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "date = str(datetime.datetime.now())\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "\n",
    "epoch = 3000\n",
    "\n",
    "for i in range(epoch):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        summary, incorrect, mean_squ_err = sess.run([merged, error, mean_square_error], {X:x, Y:y})\n",
    "        \n",
    "        print('Epoch {:4d} | incorrect {: 3.1f}% | mean squ error {: 3.1f}'.format(i + 1, incorrect * 100, mean_squ_err))\n",
    "    else:\n",
    "        summary, acc = sess.run([merged, error], {X:x, Y:y})\n",
    "\n",
    "\n",
    "    sess.run(minimize,{X:x, Y:y})\n",
    "    \n",
    "##################################################################### Application\n",
    "fw = sess.run(tf.equal(sess.run(Y, feed_dict={X:x, Y:y}),sess.run(tf.round(tf.unstack(logits, axis = 1)),feed_dict={X:x, Y:y})))\n",
    "fw = sess.run(tf.one_hot(fw, 1, axis=0))\n",
    "\n",
    "bw = sess.run(tf.equal(sess.run(Y, feed_dict={X:bx, Y:by}),sess.run(tf.round(tf.unstack(blogits, axis = 1)),feed_dict={X:bx, Y:by})))\n",
    "bw = sess.run(tf.one_hot(bw, 1, axis=0))\n",
    "\n",
    "fw = fw[0]\n",
    "fw = fw[:,[0]]\n",
    "\n",
    "bw = bw[0]\n",
    "bw = bw[:,[0]]\n",
    "\n",
    "Bi_Lstm_Output = np.column_stack((fw,bw))\n",
    "print(Bi_Lstm_Output)\n",
    "\n",
    "#######################################################################\n",
    "\n",
    "sentence = split2\n",
    "\n",
    "s_output = []\n",
    "for i in range(len(sentence)):\n",
    "    s_output.append(sentence[i])\n",
    "    if Bi_Lstm_Output[i][0] == 0:\n",
    "        s_output.append(' ')\n",
    "print(s_output)\n",
    "\n",
    "bs_output = []\n",
    "\n",
    "for i in range(len(sentence)):\n",
    "    bs_output.append(sentence[i])\n",
    "    if Bi_Lstm_Output[-i-1][1] == 0:\n",
    "        bs_output.append(' ')\n",
    "print(bs_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.8]",
   "language": "python",
   "name": "conda-env-tf1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
