{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-2f221768f650>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "#learning rate decay : 초기에는 빠르게 나중에는 천천히\n",
    "\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "mnist = read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 28,28,1])\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "lr = tf.placeholder(tf.float32)\n",
    "\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 200], stddev=0.1))\n",
    "b1 = tf.Variable(tf.ones([200])/10)\n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "b2 = tf.Variable(tf.ones([100])/10)\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([100, 60], stddev=0.1))\n",
    "b3 = tf.Variable(tf.ones([60])/10)\n",
    "Y3 = tf.nn.relu(tf.matmul(Y2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([60, 30], stddev=0.1))\n",
    "b4 = tf.Variable(tf.ones([30])/10)\n",
    "Y4 = tf.nn.relu(tf.matmul(Y3, W4) + b4)\n",
    "\n",
    "W5 = tf.Variable(tf.truncated_normal([30, 10], stddev=0.1))\n",
    "b5 = tf.Variable(tf.zeros([10]))\n",
    "Y5 = tf.matmul(Y4, W5) + b5\n",
    "\n",
    "Y = tf.nn.softmax(Y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-c9fc9e9c2b49>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y5, labels=Y_)) * 100.0\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.0005).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: acrcuracy:0.08 loss: 232.96516 (lr:0.003)\n",
      "0: ******* epoch 1 ******* test accuracy:0.0772 test loss: 232.38445\n",
      "10: acrcuracy:0.35 loss: 221.72885 (lr:0.0029855361896587787)\n",
      "20: acrcuracy:0.49 loss: 208.88753 (lr:0.0029711445178725875)\n",
      "30: acrcuracy:0.52 loss: 185.31119 (lr:0.0029568246248488817)\n",
      "40: acrcuracy:0.53 loss: 151.2927 (lr:0.0029425761525895904)\n",
      "50: acrcuracy:0.65 loss: 113.71137 (lr:0.0029283987448821647)\n",
      "50: ******* epoch 1 ******* test accuracy:0.6377 test loss: 115.7415\n",
      "60: acrcuracy:0.75 loss: 97.45021 (lr:0.0029142920472906737)\n",
      "70: acrcuracy:0.83 loss: 83.583084 (lr:0.0029002557071469426)\n",
      "80: acrcuracy:0.8 loss: 65.41127 (lr:0.0028862893735417373)\n",
      "90: acrcuracy:0.8 loss: 61.799908 (lr:0.0028723926973159898)\n",
      "100: acrcuracy:0.92 loss: 40.146263 (lr:0.0028585653310520707)\n",
      "100: ******* epoch 1 ******* test accuracy:0.8391 test loss: 54.100536\n",
      "110: acrcuracy:0.83 loss: 55.676525 (lr:0.002844806929065103)\n",
      "120: acrcuracy:0.83 loss: 61.76005 (lr:0.0028311171473943213)\n",
      "130: acrcuracy:0.92 loss: 35.456944 (lr:0.00281749564379447)\n",
      "140: acrcuracy:0.88 loss: 40.8216 (lr:0.0028039420777272502)\n",
      "150: acrcuracy:0.84 loss: 51.1276 (lr:0.0027904561103528035)\n",
      "150: ******* epoch 1 ******* test accuracy:0.8779 test loss: 41.509045\n",
      "160: acrcuracy:0.92 loss: 32.61444 (lr:0.0027770374045212438)\n",
      "170: acrcuracy:0.87 loss: 45.14465 (lr:0.0027636856247642266)\n",
      "180: acrcuracy:0.84 loss: 51.590252 (lr:0.0027504004372865616)\n",
      "190: acrcuracy:0.86 loss: 41.721165 (lr:0.002737181509957871)\n",
      "200: acrcuracy:0.86 loss: 54.293324 (lr:0.0027240285123042826)\n",
      "200: ******* epoch 1 ******* test accuracy:0.9009 test loss: 34.033997\n",
      "210: acrcuracy:0.91 loss: 28.469217 (lr:0.0027109411155001703)\n",
      "220: acrcuracy:0.92 loss: 30.864624 (lr:0.0026979189923599317)\n",
      "230: acrcuracy:0.86 loss: 39.35535 (lr:0.002684961817329811)\n",
      "240: acrcuracy:0.87 loss: 34.917824 (lr:0.0026720692664797567)\n",
      "250: acrcuracy:0.94 loss: 25.86852 (lr:0.002659241017495327)\n",
      "250: ******* epoch 1 ******* test accuracy:0.9105 test loss: 29.986092\n",
      "260: acrcuracy:0.89 loss: 33.58713 (lr:0.0026464767496696276)\n",
      "270: acrcuracy:0.86 loss: 40.92357 (lr:0.0026337761438952998)\n",
      "280: acrcuracy:0.88 loss: 44.878265 (lr:0.002621138882656537)\n",
      "290: acrcuracy:0.9 loss: 27.805576 (lr:0.0026085646500211496)\n",
      "300: acrcuracy:0.93 loss: 31.848564 (lr:0.0025960531316326675)\n",
      "300: ******* epoch 1 ******* test accuracy:0.9154 test loss: 28.311905\n",
      "310: acrcuracy:0.93 loss: 28.035133 (lr:0.002583604014702479)\n",
      "320: acrcuracy:0.92 loss: 28.186087 (lr:0.002571216988002013)\n",
      "330: acrcuracy:0.94 loss: 27.913662 (lr:0.002558891741854956)\n",
      "340: acrcuracy:0.92 loss: 31.827747 (lr:0.002546627968129513)\n",
      "350: acrcuracy:0.95 loss: 13.998662 (lr:0.0025344253602307015)\n",
      "350: ******* epoch 1 ******* test accuracy:0.9198 test loss: 26.407211\n",
      "360: acrcuracy:0.97 loss: 13.450968 (lr:0.0025222836130926888)\n",
      "370: acrcuracy:0.9 loss: 36.092506 (lr:0.0025102024231711643)\n",
      "380: acrcuracy:0.94 loss: 21.588455 (lr:0.0024981814884357505)\n",
      "390: acrcuracy:0.91 loss: 27.262093 (lr:0.0024862205083624536)\n",
      "400: acrcuracy:0.92 loss: 24.24739 (lr:0.0024743191839261473)\n",
      "400: ******* epoch 1 ******* test accuracy:0.9316 test loss: 23.51332\n",
      "410: acrcuracy:0.91 loss: 27.859583 (lr:0.002462477217593102)\n",
      "420: acrcuracy:0.95 loss: 17.286058 (lr:0.0024506943133135424)\n",
      "430: acrcuracy:0.92 loss: 22.42129 (lr:0.002438970176514248)\n",
      "440: acrcuracy:0.92 loss: 23.589556 (lr:0.0024273045140911875)\n",
      "450: acrcuracy:0.93 loss: 32.821667 (lr:0.0024156970344021934)\n",
      "450: ******* epoch 1 ******* test accuracy:0.9342 test loss: 22.391016\n",
      "460: acrcuracy:0.97 loss: 13.345158 (lr:0.0024041474472596687)\n",
      "470: acrcuracy:0.93 loss: 33.895836 (lr:0.0023926554639233334)\n",
      "480: acrcuracy:0.94 loss: 24.111078 (lr:0.002381220797093005)\n",
      "490: acrcuracy:0.88 loss: 27.996212 (lr:0.0023698431609014176)\n",
      "500: acrcuracy:0.94 loss: 19.661346 (lr:0.002358522270907074)\n",
      "500: ******* epoch 1 ******* test accuracy:0.9335 test loss: 22.398079\n",
      "510: acrcuracy:0.93 loss: 17.340813 (lr:0.002347257844087135)\n",
      "520: acrcuracy:0.92 loss: 27.79416 (lr:0.0023360495988303423)\n",
      "530: acrcuracy:0.96 loss: 16.952917 (lr:0.0023248972549299815)\n",
      "540: acrcuracy:0.96 loss: 12.7534 (lr:0.002313800533576874)\n",
      "550: acrcuracy:0.95 loss: 15.535254 (lr:0.0023027591573524086)\n",
      "550: ******* epoch 1 ******* test accuracy:0.941 test loss: 19.98932\n",
      "560: acrcuracy:0.94 loss: 18.643612 (lr:0.0022917728502216037)\n",
      "570: acrcuracy:0.94 loss: 20.029137 (lr:0.0022808413375262097)\n",
      "580: acrcuracy:0.89 loss: 28.693989 (lr:0.0022699643459778394)\n",
      "590: acrcuracy:0.89 loss: 37.08955 (lr:0.0022591416036511374)\n",
      "600: acrcuracy:0.91 loss: 25.974098 (lr:0.002248372839976982)\n",
      "600: ******* epoch 2 ******* test accuracy:0.9414 test loss: 21.248081\n",
      "610: acrcuracy:0.96 loss: 15.316749 (lr:0.0022376577857357205)\n",
      "620: acrcuracy:0.89 loss: 22.391895 (lr:0.0022269961730504387)\n",
      "630: acrcuracy:0.93 loss: 23.672235 (lr:0.0022163877353802647)\n",
      "640: acrcuracy:0.98 loss: 13.5956955 (lr:0.0022058322075137037)\n",
      "650: acrcuracy:0.94 loss: 16.491533 (lr:0.0021953293255620094)\n",
      "650: ******* epoch 2 ******* test accuracy:0.9452 test loss: 18.79299\n",
      "660: acrcuracy:0.91 loss: 26.023922 (lr:0.0021848788269525857)\n",
      "670: acrcuracy:0.97 loss: 10.795439 (lr:0.0021744804504224237)\n",
      "680: acrcuracy:0.96 loss: 22.331238 (lr:0.002164133936011568)\n",
      "690: acrcuracy:0.94 loss: 20.677505 (lr:0.00215383902505662)\n",
      "700: acrcuracy:0.94 loss: 15.171737 (lr:0.002143595460184269)\n",
      "700: ******* epoch 2 ******* test accuracy:0.9463 test loss: 18.47036\n",
      "710: acrcuracy:0.98 loss: 6.5615454 (lr:0.0021334029853048602)\n",
      "720: acrcuracy:0.95 loss: 18.95856 (lr:0.00212326134560599)\n",
      "730: acrcuracy:0.98 loss: 11.689317 (lr:0.002113170287546139)\n",
      "740: acrcuracy:0.95 loss: 14.043528 (lr:0.0021031295588483287)\n",
      "750: acrcuracy:0.95 loss: 19.950537 (lr:0.0020931389084938193)\n",
      "750: ******* epoch 2 ******* test accuracy:0.9468 test loss: 17.48723\n",
      "760: acrcuracy:0.91 loss: 34.968414 (lr:0.002083198086715832)\n",
      "770: acrcuracy:0.95 loss: 15.334159 (lr:0.002073306844993304)\n",
      "780: acrcuracy:0.98 loss: 6.8650303 (lr:0.0020634649360446776)\n",
      "790: acrcuracy:0.91 loss: 24.204502 (lr:0.0020536721138217163)\n",
      "800: acrcuracy:0.96 loss: 23.557802 (lr:0.002043928133503354)\n",
      "800: ******* epoch 2 ******* test accuracy:0.9466 test loss: 17.81293\n",
      "810: acrcuracy:0.95 loss: 14.717142 (lr:0.002034232751489576)\n",
      "820: acrcuracy:0.94 loss: 16.466967 (lr:0.0020245857253953265)\n",
      "830: acrcuracy:0.91 loss: 21.414953 (lr:0.00201498681404445)\n",
      "840: acrcuracy:0.96 loss: 19.776035 (lr:0.0020054357774636645)\n",
      "850: acrcuracy:0.97 loss: 17.731493 (lr:0.001995932376876557)\n",
      "850: ******* epoch 2 ******* test accuracy:0.9466 test loss: 18.05949\n",
      "860: acrcuracy:0.97 loss: 11.105085 (lr:0.001986476374697618)\n",
      "870: acrcuracy:0.94 loss: 18.991528 (lr:0.0019770675345263007)\n",
      "880: acrcuracy:0.99 loss: 8.314405 (lr:0.00196770562114111)\n",
      "890: acrcuracy:0.96 loss: 18.896173 (lr:0.0019583904004937245)\n",
      "900: acrcuracy:0.94 loss: 14.684488 (lr:0.001949121639703143)\n",
      "900: ******* epoch 2 ******* test accuracy:0.9494 test loss: 16.184649\n",
      "910: acrcuracy:0.92 loss: 16.559984 (lr:0.001939899107049862)\n",
      "920: acrcuracy:0.97 loss: 18.162111 (lr:0.0019307225719700854)\n",
      "930: acrcuracy:0.93 loss: 19.88152 (lr:0.0019215918050499584)\n",
      "940: acrcuracy:0.96 loss: 19.697632 (lr:0.0019125065780198325)\n",
      "950: acrcuracy:0.93 loss: 17.50296 (lr:0.0019034666637485586)\n",
      "950: ******* epoch 2 ******* test accuracy:0.9501 test loss: 16.26205\n",
      "960: acrcuracy:0.95 loss: 19.431839 (lr:0.0018944718362378086)\n",
      "970: acrcuracy:0.97 loss: 11.906616 (lr:0.001885521870616427)\n",
      "980: acrcuracy:0.94 loss: 19.235662 (lr:0.0018766165431348069)\n",
      "990: acrcuracy:0.97 loss: 8.342799 (lr:0.001867755631159297)\n",
      "1000: acrcuracy:0.98 loss: 4.975741 (lr:0.0018589389131666372)\n",
      "1000: ******* epoch 2 ******* test accuracy:0.955 test loss: 14.407242\n",
      "1010: acrcuracy:0.95 loss: 20.106306 (lr:0.0018501661687384176)\n",
      "1020: acrcuracy:0.97 loss: 10.459499 (lr:0.0018414371785555712)\n",
      "1030: acrcuracy:0.98 loss: 14.142635 (lr:0.0018327517243928889)\n",
      "1040: acrcuracy:0.95 loss: 15.507438 (lr:0.001824109589113564)\n",
      "1050: acrcuracy:0.93 loss: 20.945648 (lr:0.001815510556663764)\n",
      "1050: ******* epoch 2 ******* test accuracy:0.9544 test loss: 14.459083\n",
      "1060: acrcuracy:0.98 loss: 5.0775256 (lr:0.0018069544120672303)\n",
      "1070: acrcuracy:0.91 loss: 24.861961 (lr:0.001798440941419902)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080: acrcuracy:0.9 loss: 28.150637 (lr:0.0017899699318845701)\n",
      "1090: acrcuracy:0.97 loss: 9.911093 (lr:0.0017815411716855546)\n",
      "1100: acrcuracy:0.98 loss: 11.542522 (lr:0.0017731544501034114)\n",
      "1100: ******* epoch 2 ******* test accuracy:0.9586 test loss: 13.762108\n",
      "1110: acrcuracy:0.91 loss: 27.685589 (lr:0.0017648095574696644)\n",
      "1120: acrcuracy:0.94 loss: 15.447328 (lr:0.0017565062851615633)\n",
      "1130: acrcuracy:0.99 loss: 5.246856 (lr:0.0017482444255968678)\n",
      "1140: acrcuracy:0.91 loss: 22.327898 (lr:0.0017400237722286578)\n",
      "1150: acrcuracy:0.97 loss: 11.555874 (lr:0.0017318441195401718)\n",
      "1150: ******* epoch 2 ******* test accuracy:0.9605 test loss: 13.209354\n",
      "1160: acrcuracy:0.95 loss: 9.442512 (lr:0.001723705263039666)\n",
      "1170: acrcuracy:0.96 loss: 10.73392 (lr:0.0017156069992553045)\n",
      "1180: acrcuracy:0.97 loss: 7.7784166 (lr:0.0017075491257300707)\n",
      "1190: acrcuracy:0.96 loss: 10.338802 (lr:0.0016995314410167067)\n",
      "1200: acrcuracy:0.97 loss: 8.1813135 (lr:0.001691553744672677)\n",
      "1200: ******* epoch 3 ******* test accuracy:0.9594 test loss: 13.175504\n",
      "1210: acrcuracy:0.97 loss: 6.446677 (lr:0.0016836158372551574)\n",
      "1220: acrcuracy:0.95 loss: 22.023842 (lr:0.0016757175203160495)\n",
      "1230: acrcuracy:0.96 loss: 9.720933 (lr:0.0016678585963970183)\n",
      "1240: acrcuracy:0.93 loss: 24.49228 (lr:0.001660038869024556)\n",
      "1250: acrcuracy:0.97 loss: 16.872456 (lr:0.001652258142705072)\n",
      "1250: ******* epoch 3 ******* test accuracy:0.9593 test loss: 13.39197\n",
      "1260: acrcuracy:0.98 loss: 7.4946218 (lr:0.001644516222920002)\n",
      "1270: acrcuracy:0.96 loss: 14.24443 (lr:0.001636812916120949)\n",
      "1280: acrcuracy:0.97 loss: 7.2602124 (lr:0.001629148029724841)\n",
      "1290: acrcuracy:0.97 loss: 8.70286 (lr:0.0016215213721091197)\n",
      "1300: acrcuracy:0.92 loss: 17.94722 (lr:0.0016139327526069466)\n",
      "1300: ******* epoch 3 ******* test accuracy:0.9613 test loss: 12.875025\n",
      "1310: acrcuracy:0.96 loss: 21.011635 (lr:0.00160638198150244)\n",
      "1320: acrcuracy:0.96 loss: 12.429972 (lr:0.0015988688700259279)\n",
      "1330: acrcuracy:0.94 loss: 17.651762 (lr:0.0015913932303492327)\n",
      "1340: acrcuracy:0.98 loss: 6.2346973 (lr:0.0015839548755809732)\n",
      "1350: acrcuracy:0.93 loss: 12.471297 (lr:0.0015765536197618927)\n",
      "1350: ******* epoch 3 ******* test accuracy:0.9614 test loss: 12.318223\n",
      "1360: acrcuracy:0.99 loss: 7.2530627 (lr:0.0015691892778602098)\n",
      "1370: acrcuracy:0.97 loss: 8.6032 (lr:0.0015618616657669942)\n",
      "1380: acrcuracy:0.96 loss: 8.285086 (lr:0.0015545706002915614)\n",
      "1390: acrcuracy:0.99 loss: 7.4948354 (lr:0.0015473158991568946)\n",
      "1400: acrcuracy:0.98 loss: 10.589098 (lr:0.0015400973809950877)\n",
      "1400: ******* epoch 3 ******* test accuracy:0.9609 test loss: 12.054841\n",
      "1410: acrcuracy:0.97 loss: 7.344886 (lr:0.001532914865342811)\n",
      "1420: acrcuracy:0.97 loss: 9.347916 (lr:0.001525768172636799)\n",
      "1430: acrcuracy:0.99 loss: 6.8459125 (lr:0.0015186571242093616)\n",
      "1440: acrcuracy:0.98 loss: 8.283755 (lr:0.001511581542283918)\n",
      "1450: acrcuracy:0.96 loss: 17.52263 (lr:0.0015045412499705513)\n",
      "1450: ******* epoch 3 ******* test accuracy:0.9634 test loss: 12.102008\n",
      "1460: acrcuracy:1.0 loss: 4.3341646 (lr:0.0014975360712615872)\n",
      "1470: acrcuracy:0.97 loss: 21.23085 (lr:0.0014905658310271931)\n",
      "1480: acrcuracy:0.97 loss: 9.912791 (lr:0.0014836303550109999)\n",
      "1490: acrcuracy:0.95 loss: 11.520494 (lr:0.0014767294698257462)\n",
      "1500: acrcuracy:0.94 loss: 14.818295 (lr:0.0014698630029489428)\n",
      "1500: ******* epoch 3 ******* test accuracy:0.9596 test loss: 12.651544\n",
      "1510: acrcuracy:0.99 loss: 3.6747932 (lr:0.0014630307827185603)\n",
      "1520: acrcuracy:0.99 loss: 7.9007225 (lr:0.0014562326383287369)\n",
      "1530: acrcuracy:0.96 loss: 12.085161 (lr:0.001449468399825509)\n",
      "1540: acrcuracy:0.97 loss: 14.248982 (lr:0.0014427378981025616)\n",
      "1550: acrcuracy:0.98 loss: 8.311838 (lr:0.001436040964897001)\n",
      "1550: ******* epoch 3 ******* test accuracy:0.9645 test loss: 11.597914\n",
      "1560: acrcuracy:0.96 loss: 15.052405 (lr:0.0014293774327851483)\n",
      "1570: acrcuracy:0.97 loss: 7.555925 (lr:0.0014227471351783538)\n",
      "1580: acrcuracy:0.98 loss: 4.0515003 (lr:0.001416149906318832)\n",
      "1590: acrcuracy:0.95 loss: 12.914887 (lr:0.0014095855812755176)\n",
      "1600: acrcuracy:0.98 loss: 6.452113 (lr:0.0014030539959399427)\n",
      "1600: ******* epoch 3 ******* test accuracy:0.9611 test loss: 12.169717\n",
      "1610: acrcuracy:0.98 loss: 5.101599 (lr:0.0013965549870221337)\n",
      "1620: acrcuracy:0.95 loss: 9.057749 (lr:0.0013900883920465294)\n",
      "1630: acrcuracy:0.96 loss: 20.20814 (lr:0.0013836540493479186)\n",
      "1640: acrcuracy:0.98 loss: 6.007929 (lr:0.001377251798067398)\n",
      "1650: acrcuracy:0.96 loss: 10.802657 (lr:0.001370881478148353)\n",
      "1650: ******* epoch 3 ******* test accuracy:0.9651 test loss: 11.323734\n",
      "1660: acrcuracy:0.96 loss: 7.6466627 (lr:0.0013645429303324535)\n",
      "1670: acrcuracy:0.97 loss: 6.994004 (lr:0.0013582359961556737)\n",
      "1680: acrcuracy:0.96 loss: 11.727693 (lr:0.0013519605179443314)\n",
      "1690: acrcuracy:0.97 loss: 6.7493296 (lr:0.0013457163388111437)\n",
      "1700: acrcuracy:0.97 loss: 13.752347 (lr:0.0013395033026513076)\n",
      "1700: ******* epoch 3 ******* test accuracy:0.9637 test loss: 10.758156\n",
      "1710: acrcuracy:0.96 loss: 11.766709 (lr:0.001333321254138595)\n",
      "1720: acrcuracy:0.93 loss: 17.88446 (lr:0.0013271700387214717)\n",
      "1730: acrcuracy:0.97 loss: 9.8123865 (lr:0.0013210495026192315)\n",
      "1740: acrcuracy:0.95 loss: 19.095936 (lr:0.0013149594928181531)\n",
      "1750: acrcuracy:0.97 loss: 7.107702 (lr:0.0013088998570676745)\n",
      "1750: ******* epoch 3 ******* test accuracy:0.9635 test loss: 11.646542\n",
      "1760: acrcuracy:0.96 loss: 9.175348 (lr:0.0013028704438765861)\n",
      "1770: acrcuracy:0.97 loss: 8.214476 (lr:0.0012968711025092442)\n",
      "1780: acrcuracy:0.97 loss: 8.732914 (lr:0.001290901682981802)\n",
      "1790: acrcuracy:0.95 loss: 15.320688 (lr:0.0012849620360584606)\n",
      "1800: acrcuracy:0.96 loss: 7.310592 (lr:0.0012790520132477375)\n",
      "1800: ******* epoch 4 ******* test accuracy:0.9646 test loss: 10.966958\n",
      "1810: acrcuracy:0.98 loss: 10.138038 (lr:0.0012731714667987546)\n",
      "1820: acrcuracy:0.99 loss: 2.8050323 (lr:0.0012673202496975445)\n",
      "1830: acrcuracy:0.97 loss: 7.9775496 (lr:0.0012614982156633745)\n",
      "1840: acrcuracy:0.97 loss: 6.742136 (lr:0.0012557052191450912)\n",
      "1850: acrcuracy:0.98 loss: 6.7527575 (lr:0.0012499411153174794)\n",
      "1850: ******* epoch 4 ******* test accuracy:0.9656 test loss: 10.781455\n",
      "1860: acrcuracy:0.99 loss: 5.650897 (lr:0.0012442057600776434)\n",
      "1870: acrcuracy:0.98 loss: 5.6472454 (lr:0.0012384990100414034)\n",
      "1880: acrcuracy:0.98 loss: 8.908027 (lr:0.0012328207225397114)\n",
      "1890: acrcuracy:0.97 loss: 11.565115 (lr:0.001227170755615084)\n",
      "1900: acrcuracy:0.97 loss: 10.558859 (lr:0.0012215489680180538)\n",
      "1900: ******* epoch 4 ******* test accuracy:0.964 test loss: 11.204119\n",
      "1910: acrcuracy:0.98 loss: 4.7494965 (lr:0.001215955219203638)\n",
      "1920: acrcuracy:1.0 loss: 3.193281 (lr:0.0012103893693278251)\n",
      "1930: acrcuracy:0.97 loss: 9.578691 (lr:0.0012048512792440782)\n",
      "1940: acrcuracy:0.98 loss: 3.6442275 (lr:0.0011993408104998568)\n",
      "1950: acrcuracy:0.96 loss: 15.406355 (lr:0.0011938578253331553)\n",
      "1950: ******* epoch 4 ******* test accuracy:0.9662 test loss: 10.421147\n",
      "1960: acrcuracy:1.0 loss: 4.011755 (lr:0.001188402186669059)\n",
      "1970: acrcuracy:0.96 loss: 8.936074 (lr:0.0011829737581163168)\n",
      "1980: acrcuracy:0.98 loss: 5.696857 (lr:0.0011775724039639326)\n",
      "1990: acrcuracy:0.99 loss: 9.235943 (lr:0.0011721979891777712)\n",
      "2000: acrcuracy:0.97 loss: 12.258693 (lr:0.0011668503793971828)\n",
      "2000: ******* epoch 4 ******* test accuracy:0.9654 test loss: 10.806255\n",
      "max test accuracy: 0.9662\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0.0\n",
    "\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    \n",
    "    global max_accuracy\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    \n",
    "    max_learning_rate = 0.003\n",
    "    min_learning_rate = 0.0001\n",
    "    decay_speed = 2000.0\n",
    "    learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) *math.exp(-i/decay_speed)\n",
    "    \n",
    "    if(update_train_data):\n",
    "        a, c = sess.run([accuracy, cross_entropy], feed_dict = {X:batch_X, Y_:batch_Y})\n",
    "        print(str(i) + \": acrcuracy:\" + str(a) + \" loss: \" + str(c) + \" (lr:\" + str(learning_rate) + \")\")\n",
    "    \n",
    "    if(update_test_data):\n",
    "        a, c = sess.run([accuracy, cross_entropy], feed_dict = {X:mnist.test.images, Y_:mnist.test.labels})\n",
    "        if(a>max_accuracy):\n",
    "            max_accuracy = a\n",
    "        print(str(i) + \": ******* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ******* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "    sess.run (train_step, feed_dict={X: batch_X, Y_: batch_Y, lr: learning_rate})\n",
    "\n",
    "for i in range(2000+1):\n",
    "    training_step(i, i%50==0, i%10 == 0)\n",
    "print(\"max test accuracy: \" + str(max_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.8]",
   "language": "python",
   "name": "conda-env-tf1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
