{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "import math\n",
    "graph = tf.Graph()\n",
    "session = tf.InteractiveSession(graph=graph)\n",
    "\n",
    "mnist = read_data_sets(\"data\", one_hot=True, reshape=False)\n",
    "\n",
    "\"parameter\"\n",
    "epoch = 15\n",
    "batch_size = 100\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "lr = tf.placeholder(tf.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "X2 = tf.reshape(X, [-1, 784])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([784, 512], stddev=0.1))\n",
    "b1 = tf.Variable(tf.random_normal([512], stddev=-0.1))\n",
    "L1 = tf.nn.relu(tf.matmul(X2, W1)+b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([512, 256], stddev=0.1))\n",
    "b2 = tf.Variable(tf.random_normal([256], stddev=-0.1))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2)+b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 128], stddev=0.1))\n",
    "b3 = tf.Variable(tf.random_normal([128], stddev=-0.1))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3)+b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([128, 64], stddev=0.1))\n",
    "b4 = tf.Variable(tf.random_normal([64], stddev=-0.1))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4)+b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.Variable(tf.random_normal([64, 10], stddev=0.1))\n",
    "b5 = tf.Variable(tf.random_normal([10], stddev=-0.1))\n",
    "logits = tf.matmul(L4, W5)+b5\n",
    "H = tf.nn.softmax(logits)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))*100\n",
    "train = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(tf.argmax(H, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "c = 0\n",
    "avg_cost = 0\n",
    "for e in range(epoch):\n",
    "    total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    global avg_cost\n",
    "    global c\n",
    "    print(e+1)\n",
    "    for i in range(total_batch):\n",
    "        \n",
    "        max_learning_rate = 0.003\n",
    "        min_learning_rate = 0.0001\n",
    "        decay_speed = 2000.0\n",
    "        learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) *math.exp(-i/decay_speed)\n",
    "        \n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        sess.run(train, feed_dict={X:batch_xs, Y:batch_ys, keep_prob:0.7, lr:learning_rate})\n",
    "        avg_cost += c/total_batch\n",
    "    a, c = sess.run([accuracy, cost], feed_dict={X:mnist.test.images, Y:mnist.test.labels, keep_prob:1, lr:learning_rate})\n",
    "    print(a,c, avg_cost/(e+1), learning_rate)\n",
    "    \n",
    "writer = tf.summary.FileWriter('sample2',session.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.28655723\n",
      "1.0 0.0002927937\n",
      "1.0 7.9128906e-05\n",
      "1.0 3.4050317e-05\n",
      "1.0 1.7268972e-05\n",
      "1.0 9.466743e-06\n",
      "1.0 5.4092657e-06\n",
      "1.0 3.164304e-06\n",
      "1.0 1.8767023e-06\n",
      "1.0 1.1224008e-06\n",
      "1.0 6.7458024e-07\n",
      "[0, 0] [0.00071413]\n",
      "[0, 1] [0.99926037]\n",
      "[1, 0] [0.99926025]\n",
      "[1, 1] [0.00104597]\n"
     ]
    }
   ],
   "source": [
    "xdata = [[0,0], [0,1], [1,0], [1,1]]\n",
    "ydata = [[0], [1], [1], [0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2,2], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([2]))\n",
    "L1 = tf.nn.sigmoid(tf.matmul(X, W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([2,1]))\n",
    "b2 = tf.Variable(tf.zeros([1]))\n",
    "logits = tf.matmul(L1, W2)+b2\n",
    "H = tf.nn.sigmoid(logits)\n",
    "H2 = tf.round(H)\n",
    "\n",
    "cost = tf.losses.mean_squared_error(H, Y)\n",
    "train = tf.train.AdamOptimizer(0.05).minimize(cost)\n",
    "\n",
    "is_correct = tf.equal(H2, Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(10001):\n",
    "    sess.run(train, feed_dict={X:xdata, Y:ydata})\n",
    "    if i%1000 == 0:\n",
    "        a, c = sess.run([accuracy, cost], feed_dict={X:xdata, Y:ydata})\n",
    "        print(a, c)\n",
    "y = sess.run(H, feed_dict={X:xdata})\n",
    "for i in range(4):\n",
    "    print(xdata[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.8]",
   "language": "python",
   "name": "conda-env-tf1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
