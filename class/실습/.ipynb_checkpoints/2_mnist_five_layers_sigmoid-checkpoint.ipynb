{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-64bac15a2a11>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "mnist = read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 28,28,1])\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 200], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([200]))\n",
    "Y1 = tf.nn.sigmoid(tf.matmul(XX, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([100]))\n",
    "Y2 = tf.nn.sigmoid(tf.matmul(Y1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([100, 60], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([60]))\n",
    "Y3 = tf.nn.sigmoid(tf.matmul(Y2, W3) + b3)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([60, 30], stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([30]))\n",
    "Y4 = tf.nn.sigmoid(tf.matmul(Y3, W4) + b4)\n",
    "\n",
    "W5 = tf.Variable(tf.truncated_normal([30, 10], stddev=0.1))\n",
    "b5 = tf.Variable(tf.zeros([10]))\n",
    "Y5 = tf.matmul(Y4, W5) + b5\n",
    "\n",
    "Y = tf.nn.softmax(Y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-088ea9850063>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y5, labels=Y_)) * 100.0\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.003).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: acrcuracy:0.08 loss: 237.03053\n",
      "0: ******* epoch 1 ******* test accuracy:0.1028 test loss: 235.23\n",
      "10: acrcuracy:0.07 loss: 231.33566\n",
      "20: acrcuracy:0.15 loss: 226.65854\n",
      "30: acrcuracy:0.4 loss: 220.89262\n",
      "40: acrcuracy:0.31 loss: 207.89478\n",
      "50: acrcuracy:0.32 loss: 191.10434\n",
      "50: ******* epoch 1 ******* test accuracy:0.3326 test loss: 191.8341\n",
      "60: acrcuracy:0.42 loss: 178.57481\n",
      "70: acrcuracy:0.57 loss: 155.02377\n",
      "80: acrcuracy:0.55 loss: 141.05423\n",
      "90: acrcuracy:0.66 loss: 130.06491\n",
      "100: acrcuracy:0.67 loss: 112.06565\n",
      "100: ******* epoch 1 ******* test accuracy:0.6738 test loss: 120.081436\n",
      "110: acrcuracy:0.62 loss: 118.32031\n",
      "120: acrcuracy:0.72 loss: 115.21548\n",
      "130: acrcuracy:0.7 loss: 89.44031\n",
      "140: acrcuracy:0.83 loss: 88.71358\n",
      "150: acrcuracy:0.72 loss: 103.35893\n",
      "150: ******* epoch 1 ******* test accuracy:0.7243 test loss: 90.03836\n",
      "160: acrcuracy:0.75 loss: 86.33683\n",
      "170: acrcuracy:0.71 loss: 83.28438\n",
      "180: acrcuracy:0.75 loss: 84.390945\n",
      "190: acrcuracy:0.73 loss: 86.09332\n",
      "200: acrcuracy:0.76 loss: 83.50853\n",
      "200: ******* epoch 1 ******* test accuracy:0.7874 test loss: 77.18025\n",
      "210: acrcuracy:0.8 loss: 74.114716\n",
      "220: acrcuracy:0.79 loss: 83.92227\n",
      "230: acrcuracy:0.76 loss: 84.93171\n",
      "240: acrcuracy:0.8 loss: 75.264694\n",
      "250: acrcuracy:0.85 loss: 55.506958\n",
      "250: ******* epoch 1 ******* test accuracy:0.8137 test loss: 63.548363\n",
      "260: acrcuracy:0.8 loss: 61.4196\n",
      "270: acrcuracy:0.8 loss: 72.33703\n",
      "280: acrcuracy:0.86 loss: 57.165367\n",
      "290: acrcuracy:0.87 loss: 56.830383\n",
      "300: acrcuracy:0.87 loss: 49.015667\n",
      "300: ******* epoch 1 ******* test accuracy:0.8286 test loss: 55.722343\n",
      "310: acrcuracy:0.88 loss: 52.02263\n",
      "320: acrcuracy:0.86 loss: 64.29598\n",
      "330: acrcuracy:0.92 loss: 42.773617\n",
      "340: acrcuracy:0.9 loss: 46.227962\n",
      "350: acrcuracy:0.93 loss: 34.98012\n",
      "350: ******* epoch 1 ******* test accuracy:0.8949 test loss: 43.968258\n",
      "360: acrcuracy:0.94 loss: 30.811987\n",
      "370: acrcuracy:0.88 loss: 49.442856\n",
      "380: acrcuracy:0.94 loss: 28.59285\n",
      "390: acrcuracy:0.92 loss: 36.85552\n",
      "400: acrcuracy:0.92 loss: 47.237926\n",
      "400: ******* epoch 1 ******* test accuracy:0.9028 test loss: 39.94033\n",
      "410: acrcuracy:0.87 loss: 42.29266\n",
      "420: acrcuracy:0.95 loss: 24.161427\n",
      "430: acrcuracy:0.91 loss: 33.15794\n",
      "440: acrcuracy:0.9 loss: 32.99171\n",
      "450: acrcuracy:0.91 loss: 34.66824\n",
      "450: ******* epoch 1 ******* test accuracy:0.9209 test loss: 32.947933\n",
      "460: acrcuracy:0.92 loss: 29.359222\n",
      "470: acrcuracy:0.91 loss: 40.38026\n",
      "480: acrcuracy:0.92 loss: 26.233822\n",
      "490: acrcuracy:0.9 loss: 37.7577\n",
      "500: acrcuracy:0.94 loss: 27.115717\n",
      "500: ******* epoch 1 ******* test accuracy:0.9288 test loss: 29.607891\n",
      "510: acrcuracy:0.95 loss: 24.525879\n",
      "520: acrcuracy:0.88 loss: 40.92929\n",
      "530: acrcuracy:0.94 loss: 24.310556\n",
      "540: acrcuracy:0.95 loss: 20.384842\n",
      "550: acrcuracy:0.96 loss: 16.97989\n",
      "550: ******* epoch 1 ******* test accuracy:0.9283 test loss: 27.916744\n",
      "560: acrcuracy:0.91 loss: 21.997879\n",
      "570: acrcuracy:0.94 loss: 20.148443\n",
      "580: acrcuracy:0.92 loss: 34.351974\n",
      "590: acrcuracy:0.88 loss: 47.638916\n",
      "600: acrcuracy:0.9 loss: 37.47317\n",
      "600: ******* epoch 2 ******* test accuracy:0.9311 test loss: 27.589804\n",
      "610: acrcuracy:0.95 loss: 23.06442\n",
      "620: acrcuracy:0.92 loss: 30.253025\n",
      "630: acrcuracy:0.93 loss: 27.141548\n",
      "640: acrcuracy:0.94 loss: 23.232313\n",
      "650: acrcuracy:0.94 loss: 19.335556\n",
      "650: ******* epoch 2 ******* test accuracy:0.9412 test loss: 23.565998\n",
      "660: acrcuracy:0.97 loss: 16.321075\n",
      "670: acrcuracy:0.96 loss: 16.989367\n",
      "680: acrcuracy:0.96 loss: 20.112234\n",
      "690: acrcuracy:0.95 loss: 23.717522\n",
      "700: acrcuracy:0.96 loss: 16.046408\n",
      "700: ******* epoch 2 ******* test accuracy:0.9467 test loss: 21.803726\n",
      "710: acrcuracy:1.0 loss: 5.7724876\n",
      "720: acrcuracy:0.97 loss: 18.322353\n",
      "730: acrcuracy:0.94 loss: 17.112507\n",
      "740: acrcuracy:0.97 loss: 16.394144\n",
      "750: acrcuracy:0.95 loss: 15.803495\n",
      "750: ******* epoch 2 ******* test accuracy:0.9463 test loss: 21.464645\n",
      "760: acrcuracy:0.92 loss: 38.080853\n",
      "770: acrcuracy:0.95 loss: 15.997163\n",
      "780: acrcuracy:0.94 loss: 18.741215\n",
      "790: acrcuracy:0.88 loss: 31.6048\n",
      "800: acrcuracy:0.94 loss: 22.430414\n",
      "800: ******* epoch 2 ******* test accuracy:0.9391 test loss: 23.168077\n",
      "810: acrcuracy:0.96 loss: 18.566074\n",
      "820: acrcuracy:0.94 loss: 26.676079\n",
      "830: acrcuracy:0.93 loss: 24.144558\n",
      "840: acrcuracy:0.97 loss: 17.316721\n",
      "850: acrcuracy:0.97 loss: 15.3501835\n",
      "850: ******* epoch 2 ******* test accuracy:0.9431 test loss: 21.687336\n",
      "860: acrcuracy:0.98 loss: 9.603727\n",
      "870: acrcuracy:0.96 loss: 23.949572\n",
      "880: acrcuracy:0.96 loss: 10.979002\n",
      "890: acrcuracy:0.92 loss: 25.15136\n",
      "900: acrcuracy:0.95 loss: 16.592066\n",
      "900: ******* epoch 2 ******* test accuracy:0.9536 test loss: 18.462603\n",
      "910: acrcuracy:0.97 loss: 15.331395\n",
      "920: acrcuracy:0.97 loss: 14.167568\n",
      "930: acrcuracy:0.94 loss: 17.288567\n",
      "940: acrcuracy:0.96 loss: 21.246758\n",
      "950: acrcuracy:0.96 loss: 16.09185\n",
      "950: ******* epoch 2 ******* test accuracy:0.9492 test loss: 19.463684\n",
      "960: acrcuracy:0.94 loss: 22.191935\n",
      "970: acrcuracy:0.97 loss: 12.898501\n",
      "980: acrcuracy:0.94 loss: 24.829775\n",
      "990: acrcuracy:0.98 loss: 11.671746\n",
      "1000: acrcuracy:0.96 loss: 10.763156\n",
      "1000: ******* epoch 2 ******* test accuracy:0.9543 test loss: 17.866037\n",
      "1010: acrcuracy:0.94 loss: 20.839725\n",
      "1020: acrcuracy:0.96 loss: 15.072701\n",
      "1030: acrcuracy:0.95 loss: 14.582701\n",
      "1040: acrcuracy:0.96 loss: 17.78781\n",
      "1050: acrcuracy:0.92 loss: 32.3583\n",
      "1050: ******* epoch 2 ******* test accuracy:0.9576 test loss: 16.226574\n",
      "1060: acrcuracy:0.98 loss: 12.192156\n",
      "1070: acrcuracy:0.96 loss: 14.450814\n",
      "1080: acrcuracy:0.87 loss: 48.516727\n",
      "1090: acrcuracy:0.96 loss: 15.373566\n",
      "1100: acrcuracy:0.98 loss: 10.088168\n",
      "1100: ******* epoch 2 ******* test accuracy:0.9571 test loss: 16.883448\n",
      "1110: acrcuracy:0.94 loss: 24.873825\n",
      "1120: acrcuracy:0.97 loss: 13.099294\n",
      "1130: acrcuracy:0.98 loss: 6.2261763\n",
      "1140: acrcuracy:0.95 loss: 19.22627\n",
      "1150: acrcuracy:0.96 loss: 16.116177\n",
      "1150: ******* epoch 2 ******* test accuracy:0.9588 test loss: 15.930975\n",
      "1160: acrcuracy:0.97 loss: 12.160831\n",
      "1170: acrcuracy:0.96 loss: 11.414571\n",
      "1180: acrcuracy:0.99 loss: 5.8147087\n",
      "1190: acrcuracy:0.99 loss: 11.897613\n",
      "1200: acrcuracy:0.95 loss: 14.795073\n",
      "1200: ******* epoch 3 ******* test accuracy:0.9577 test loss: 15.926661\n",
      "1210: acrcuracy:0.98 loss: 16.191538\n",
      "1220: acrcuracy:0.96 loss: 20.458092\n",
      "1230: acrcuracy:0.96 loss: 7.570259\n",
      "1240: acrcuracy:0.95 loss: 17.109894\n",
      "1250: acrcuracy:0.97 loss: 16.598236\n",
      "1250: ******* epoch 3 ******* test accuracy:0.9599 test loss: 15.524183\n",
      "1260: acrcuracy:0.99 loss: 5.876786\n",
      "1270: acrcuracy:0.96 loss: 14.891153\n",
      "1280: acrcuracy:0.99 loss: 5.4728584\n",
      "1290: acrcuracy:0.96 loss: 8.155456\n",
      "1300: acrcuracy:0.94 loss: 20.055893\n",
      "1300: ******* epoch 3 ******* test accuracy:0.9586 test loss: 14.98229\n",
      "1310: acrcuracy:0.96 loss: 18.780397\n",
      "1320: acrcuracy:0.97 loss: 14.218518\n",
      "1330: acrcuracy:0.95 loss: 10.830275\n",
      "1340: acrcuracy:0.98 loss: 6.9857664\n",
      "1350: acrcuracy:0.92 loss: 15.030212\n",
      "1350: ******* epoch 3 ******* test accuracy:0.9607 test loss: 14.62618\n",
      "1360: acrcuracy:0.98 loss: 9.462166\n",
      "1370: acrcuracy:0.99 loss: 6.168203\n",
      "1380: acrcuracy:0.96 loss: 11.861656\n",
      "1390: acrcuracy:0.98 loss: 9.515449\n",
      "1400: acrcuracy:0.99 loss: 8.830289\n",
      "1400: ******* epoch 3 ******* test accuracy:0.9647 test loss: 13.653709\n",
      "1410: acrcuracy:0.98 loss: 5.757425\n",
      "1420: acrcuracy:0.99 loss: 6.1617284\n",
      "1430: acrcuracy:0.97 loss: 9.244523\n",
      "1440: acrcuracy:0.97 loss: 13.275424\n",
      "1450: acrcuracy:0.96 loss: 20.035036\n",
      "1450: ******* epoch 3 ******* test accuracy:0.9618 test loss: 14.59984\n",
      "1460: acrcuracy:0.98 loss: 5.4175324\n",
      "1470: acrcuracy:0.94 loss: 18.990917\n",
      "1480: acrcuracy:0.98 loss: 7.3574214\n",
      "1490: acrcuracy:0.98 loss: 11.197957\n",
      "1500: acrcuracy:0.98 loss: 7.301352\n",
      "1500: ******* epoch 3 ******* test accuracy:0.9594 test loss: 15.661551\n",
      "1510: acrcuracy:0.99 loss: 4.6668844\n",
      "1520: acrcuracy:0.99 loss: 5.6610403\n",
      "1530: acrcuracy:0.99 loss: 7.109981\n",
      "1540: acrcuracy:0.97 loss: 16.609344\n",
      "1550: acrcuracy:0.97 loss: 14.491743\n",
      "1550: ******* epoch 3 ******* test accuracy:0.9635 test loss: 14.006387\n",
      "1560: acrcuracy:0.96 loss: 12.344883\n",
      "1570: acrcuracy:0.97 loss: 11.719328\n",
      "1580: acrcuracy:1.0 loss: 3.8844872\n",
      "1590: acrcuracy:0.96 loss: 14.722556\n",
      "1600: acrcuracy:0.98 loss: 6.171691\n",
      "1600: ******* epoch 3 ******* test accuracy:0.9641 test loss: 13.503565\n",
      "1610: acrcuracy:0.98 loss: 5.7488303\n",
      "1620: acrcuracy:0.98 loss: 9.736365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1630: acrcuracy:0.96 loss: 12.407457\n",
      "1640: acrcuracy:0.98 loss: 9.881744\n",
      "1650: acrcuracy:0.98 loss: 9.161419\n",
      "1650: ******* epoch 3 ******* test accuracy:0.9634 test loss: 14.075383\n",
      "1660: acrcuracy:0.96 loss: 8.626036\n",
      "1670: acrcuracy:0.97 loss: 7.641519\n",
      "1680: acrcuracy:0.98 loss: 6.167442\n",
      "1690: acrcuracy:1.0 loss: 3.5015657\n",
      "1700: acrcuracy:0.97 loss: 20.014132\n",
      "1700: ******* epoch 3 ******* test accuracy:0.968 test loss: 12.436061\n",
      "1710: acrcuracy:0.97 loss: 9.262731\n",
      "1720: acrcuracy:0.95 loss: 13.884703\n",
      "1730: acrcuracy:0.97 loss: 11.141541\n",
      "1740: acrcuracy:0.96 loss: 14.10688\n",
      "1750: acrcuracy:0.98 loss: 7.551283\n",
      "1750: ******* epoch 3 ******* test accuracy:0.9591 test loss: 15.175753\n",
      "1760: acrcuracy:0.97 loss: 9.770658\n",
      "1770: acrcuracy:0.97 loss: 12.648958\n",
      "1780: acrcuracy:0.96 loss: 13.816847\n",
      "1790: acrcuracy:0.98 loss: 7.2550526\n",
      "1800: acrcuracy:0.99 loss: 4.6536202\n",
      "1800: ******* epoch 4 ******* test accuracy:0.9687 test loss: 12.034727\n",
      "1810: acrcuracy:0.98 loss: 9.748783\n",
      "1820: acrcuracy:0.99 loss: 3.001309\n",
      "1830: acrcuracy:0.97 loss: 5.7178183\n",
      "1840: acrcuracy:0.98 loss: 6.9210863\n",
      "1850: acrcuracy:0.97 loss: 12.154917\n",
      "1850: ******* epoch 4 ******* test accuracy:0.9677 test loss: 12.343288\n",
      "1860: acrcuracy:0.97 loss: 7.292545\n",
      "1870: acrcuracy:0.97 loss: 6.642816\n",
      "1880: acrcuracy:0.97 loss: 8.677165\n",
      "1890: acrcuracy:0.96 loss: 12.94886\n",
      "1900: acrcuracy:0.97 loss: 12.520117\n",
      "1900: ******* epoch 4 ******* test accuracy:0.9641 test loss: 13.70305\n",
      "1910: acrcuracy:0.98 loss: 6.7381186\n",
      "1920: acrcuracy:0.99 loss: 3.460933\n",
      "1930: acrcuracy:0.97 loss: 11.448696\n",
      "1940: acrcuracy:0.98 loss: 5.920239\n",
      "1950: acrcuracy:0.96 loss: 17.460802\n",
      "1950: ******* epoch 4 ******* test accuracy:0.9672 test loss: 12.988438\n",
      "1960: acrcuracy:1.0 loss: 1.9345393\n",
      "1970: acrcuracy:0.99 loss: 4.7293897\n",
      "1980: acrcuracy:0.98 loss: 8.363131\n",
      "1990: acrcuracy:0.97 loss: 11.761871\n",
      "2000: acrcuracy:0.98 loss: 12.541977\n",
      "2000: ******* epoch 4 ******* test accuracy:0.9685 test loss: 11.816121\n",
      "max test accuracy: 0.9687\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0.0\n",
    "\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    \n",
    "    global max_accuracy\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    \n",
    "    if(update_train_data):\n",
    "        a, c = sess.run([accuracy, cross_entropy], feed_dict = {X:batch_X, Y_:batch_Y})\n",
    "        print(str(i) + \": acrcuracy:\" + str(a) + \" loss: \" + str(c))\n",
    "    \n",
    "    if(update_test_data):\n",
    "        a, c = sess.run([accuracy, cross_entropy], feed_dict = {X:mnist.test.images, Y_:mnist.test.labels})\n",
    "        if(a>max_accuracy):\n",
    "            max_accuracy = a\n",
    "        print(str(i) + \": ******* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ******* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "    sess.run (train_step, feed_dict={X: batch_X, Y_: batch_Y})\n",
    "\n",
    "for i in range(2000+1):\n",
    "    training_step(i, i%50==0, i%10 == 0)\n",
    "print(\"max test accuracy: \" + str(max_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.8]",
   "language": "python",
   "name": "conda-env-tf1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
