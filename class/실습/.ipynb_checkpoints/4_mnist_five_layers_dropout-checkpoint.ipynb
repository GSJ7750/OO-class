{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-785275be4a23>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "mnist = read_data_sets(\"data\", one_hot=True, reshape=False, validation_size=0)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 28,28,1])\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "lr = tf.placeholder(tf.float32)\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "XX = tf.reshape(X, [-1, 784])\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([784, 200], stddev=0.1))\n",
    "b1 = tf.Variable(tf.ones([200])/10) #다 0.1로 초기화\n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + b1)\n",
    "Y1d = tf.nn.dropout(Y1, pkeep)#pkeep : 각 요소가 keep될 확률\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([200, 100], stddev=0.1))\n",
    "b2 = tf.Variable(tf.ones([100])/10)\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1, W2) + b2)\n",
    "Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([100, 60], stddev=0.1))\n",
    "b3 = tf.Variable(tf.ones([60])/10)\n",
    "Y3 = tf.nn.relu(tf.matmul(Y2, W3) + b3)\n",
    "Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([60, 30], stddev=0.1))\n",
    "b4 = tf.Variable(tf.ones([30])/10)\n",
    "Y4 = tf.nn.relu(tf.matmul(Y3, W4) + b4)\n",
    "Y4d = tf.nn.dropout(Y4, pkeep)\n",
    "\n",
    "W5 = tf.Variable(tf.truncated_normal([30, 10], stddev=0.1))\n",
    "b5 = tf.Variable(tf.zeros([10]))\n",
    "Y5 = tf.matmul(Y4, W5) + b5\n",
    "\n",
    "Y = tf.nn.softmax(Y5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-5f14846edf3f>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Y5, labels=Y_)) * 100.0\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tf_1.8\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: acrcuracy:0.12 loss: 230.02202 (lr:0.003)\n",
      "0: ******* epoch 1 ******* test accuracy:0.0859 test loss: 230.40216\n",
      "10: acrcuracy:0.58 loss: 128.90306 (lr:0.0029855361896587787)\n",
      "20: acrcuracy:0.79 loss: 83.32968 (lr:0.0029711445178725875)\n",
      "30: acrcuracy:0.84 loss: 61.99081 (lr:0.0029568246248488817)\n",
      "40: acrcuracy:0.8 loss: 49.917355 (lr:0.0029425761525895904)\n",
      "50: acrcuracy:0.86 loss: 41.400257 (lr:0.0029283987448821647)\n",
      "50: ******* epoch 1 ******* test accuracy:0.8771 test loss: 40.778866\n",
      "60: acrcuracy:0.88 loss: 39.165306 (lr:0.0029142920472906737)\n",
      "70: acrcuracy:0.93 loss: 40.1728 (lr:0.0029002557071469426)\n",
      "80: acrcuracy:0.88 loss: 35.368164 (lr:0.0028862893735417373)\n",
      "90: acrcuracy:0.89 loss: 32.966812 (lr:0.0028723926973159898)\n",
      "100: acrcuracy:0.99 loss: 11.038314 (lr:0.0028585653310520707)\n",
      "100: ******* epoch 1 ******* test accuracy:0.9203 test loss: 26.993938\n",
      "110: acrcuracy:0.88 loss: 38.544464 (lr:0.002844806929065103)\n",
      "120: acrcuracy:0.87 loss: 49.529137 (lr:0.0028311171473943213)\n",
      "130: acrcuracy:0.96 loss: 19.19324 (lr:0.00281749564379447)\n",
      "140: acrcuracy:0.95 loss: 22.80856 (lr:0.0028039420777272502)\n",
      "150: acrcuracy:0.92 loss: 22.42692 (lr:0.0027904561103528035)\n",
      "150: ******* epoch 1 ******* test accuracy:0.9298 test loss: 23.493555\n",
      "160: acrcuracy:0.94 loss: 18.816473 (lr:0.0027770374045212438)\n",
      "170: acrcuracy:0.92 loss: 23.380116 (lr:0.0027636856247642266)\n",
      "180: acrcuracy:0.89 loss: 33.464817 (lr:0.0027504004372865616)\n",
      "190: acrcuracy:0.92 loss: 23.214857 (lr:0.002737181509957871)\n",
      "200: acrcuracy:0.92 loss: 36.01352 (lr:0.0027240285123042826)\n",
      "200: ******* epoch 1 ******* test accuracy:0.9323 test loss: 22.259653\n",
      "210: acrcuracy:0.96 loss: 16.115566 (lr:0.0027109411155001703)\n",
      "220: acrcuracy:0.93 loss: 31.201464 (lr:0.0026979189923599317)\n",
      "230: acrcuracy:0.88 loss: 35.627293 (lr:0.002684961817329811)\n",
      "240: acrcuracy:0.93 loss: 22.465508 (lr:0.0026720692664797567)\n",
      "250: acrcuracy:0.97 loss: 15.518511 (lr:0.002659241017495327)\n",
      "250: ******* epoch 1 ******* test accuracy:0.9414 test loss: 18.458385\n",
      "260: acrcuracy:0.97 loss: 14.167341 (lr:0.0026464767496696276)\n",
      "270: acrcuracy:0.9 loss: 26.281345 (lr:0.0026337761438952998)\n",
      "280: acrcuracy:0.93 loss: 22.574741 (lr:0.002621138882656537)\n",
      "290: acrcuracy:0.94 loss: 20.325386 (lr:0.0026085646500211496)\n",
      "300: acrcuracy:0.95 loss: 24.6399 (lr:0.0025960531316326675)\n",
      "300: ******* epoch 1 ******* test accuracy:0.9454 test loss: 17.615566\n",
      "310: acrcuracy:0.91 loss: 22.098392 (lr:0.002583604014702479)\n",
      "320: acrcuracy:0.95 loss: 19.308567 (lr:0.002571216988002013)\n",
      "330: acrcuracy:0.97 loss: 13.94407 (lr:0.002558891741854956)\n",
      "340: acrcuracy:0.92 loss: 19.29536 (lr:0.002546627968129513)\n",
      "350: acrcuracy:0.98 loss: 9.149549 (lr:0.0025344253602307015)\n",
      "350: ******* epoch 1 ******* test accuracy:0.9496 test loss: 16.354816\n",
      "360: acrcuracy:0.98 loss: 10.44073 (lr:0.0025222836130926888)\n",
      "370: acrcuracy:0.9 loss: 23.585154 (lr:0.0025102024231711643)\n",
      "380: acrcuracy:0.95 loss: 14.247842 (lr:0.0024981814884357505)\n",
      "390: acrcuracy:0.92 loss: 22.875635 (lr:0.0024862205083624536)\n",
      "400: acrcuracy:0.94 loss: 16.816107 (lr:0.0024743191839261473)\n",
      "400: ******* epoch 1 ******* test accuracy:0.9496 test loss: 16.234295\n",
      "410: acrcuracy:0.94 loss: 17.099716 (lr:0.002462477217593102)\n",
      "420: acrcuracy:0.97 loss: 10.318515 (lr:0.0024506943133135424)\n",
      "430: acrcuracy:0.96 loss: 12.400407 (lr:0.002438970176514248)\n",
      "440: acrcuracy:0.98 loss: 9.328798 (lr:0.0024273045140911875)\n",
      "450: acrcuracy:0.96 loss: 16.49383 (lr:0.0024156970344021934)\n",
      "450: ******* epoch 1 ******* test accuracy:0.958 test loss: 14.238344\n",
      "460: acrcuracy:0.95 loss: 15.813768 (lr:0.0024041474472596687)\n",
      "470: acrcuracy:0.96 loss: 17.109825 (lr:0.0023926554639233334)\n",
      "480: acrcuracy:0.95 loss: 11.928839 (lr:0.002381220797093005)\n",
      "490: acrcuracy:0.93 loss: 15.109552 (lr:0.0023698431609014176)\n",
      "500: acrcuracy:0.91 loss: 17.29738 (lr:0.002358522270907074)\n",
      "500: ******* epoch 1 ******* test accuracy:0.9519 test loss: 16.464682\n",
      "510: acrcuracy:0.97 loss: 11.044043 (lr:0.002347257844087135)\n",
      "520: acrcuracy:0.97 loss: 17.176836 (lr:0.0023360495988303423)\n",
      "530: acrcuracy:0.99 loss: 9.430374 (lr:0.0023248972549299815)\n",
      "540: acrcuracy:0.98 loss: 8.40935 (lr:0.002313800533576874)\n",
      "550: acrcuracy:0.95 loss: 10.513412 (lr:0.0023027591573524086)\n",
      "550: ******* epoch 1 ******* test accuracy:0.961 test loss: 12.898413\n",
      "560: acrcuracy:0.96 loss: 10.887012 (lr:0.0022917728502216037)\n",
      "570: acrcuracy:0.97 loss: 11.491423 (lr:0.0022808413375262097)\n",
      "580: acrcuracy:0.96 loss: 16.943531 (lr:0.0022699643459778394)\n",
      "590: acrcuracy:0.93 loss: 31.542646 (lr:0.0022591416036511374)\n",
      "600: acrcuracy:0.93 loss: 15.77794 (lr:0.002248372839976982)\n",
      "600: ******* epoch 2 ******* test accuracy:0.9548 test loss: 15.142129\n",
      "610: acrcuracy:0.96 loss: 13.104454 (lr:0.0022376577857357205)\n",
      "620: acrcuracy:0.97 loss: 11.624885 (lr:0.0022269961730504387)\n",
      "630: acrcuracy:0.96 loss: 11.213573 (lr:0.0022163877353802647)\n",
      "640: acrcuracy:0.97 loss: 7.649365 (lr:0.0022058322075137037)\n",
      "650: acrcuracy:0.95 loss: 10.728038 (lr:0.0021953293255620094)\n",
      "650: ******* epoch 2 ******* test accuracy:0.9608 test loss: 12.983039\n",
      "660: acrcuracy:0.96 loss: 11.457066 (lr:0.0021848788269525857)\n",
      "670: acrcuracy:0.98 loss: 7.967938 (lr:0.0021744804504224237)\n",
      "680: acrcuracy:0.96 loss: 10.282534 (lr:0.002164133936011568)\n",
      "690: acrcuracy:0.97 loss: 10.596062 (lr:0.00215383902505662)\n",
      "700: acrcuracy:0.96 loss: 14.443243 (lr:0.002143595460184269)\n",
      "700: ******* epoch 2 ******* test accuracy:0.967 test loss: 10.724375\n",
      "710: acrcuracy:1.0 loss: 1.3508273 (lr:0.0021334029853048602)\n",
      "720: acrcuracy:0.97 loss: 8.001062 (lr:0.00212326134560599)\n",
      "730: acrcuracy:0.98 loss: 11.347613 (lr:0.002113170287546139)\n",
      "740: acrcuracy:0.98 loss: 6.11 (lr:0.0021031295588483287)\n",
      "750: acrcuracy:0.98 loss: 7.157001 (lr:0.0020931389084938193)\n",
      "750: ******* epoch 2 ******* test accuracy:0.9657 test loss: 11.150622\n",
      "760: acrcuracy:0.95 loss: 18.660154 (lr:0.002083198086715832)\n",
      "770: acrcuracy:0.98 loss: 7.2253585 (lr:0.002073306844993304)\n",
      "780: acrcuracy:0.97 loss: 7.253724 (lr:0.0020634649360446776)\n",
      "790: acrcuracy:0.97 loss: 14.166278 (lr:0.0020536721138217163)\n",
      "800: acrcuracy:0.96 loss: 22.029852 (lr:0.002043928133503354)\n",
      "800: ******* epoch 2 ******* test accuracy:0.9618 test loss: 12.674946\n",
      "810: acrcuracy:0.97 loss: 12.351169 (lr:0.002034232751489576)\n",
      "820: acrcuracy:0.94 loss: 15.652856 (lr:0.0020245857253953265)\n",
      "830: acrcuracy:0.97 loss: 9.52526 (lr:0.00201498681404445)\n",
      "840: acrcuracy:0.98 loss: 9.857496 (lr:0.0020054357774636645)\n",
      "850: acrcuracy:0.98 loss: 10.962162 (lr:0.001995932376876557)\n",
      "850: ******* epoch 2 ******* test accuracy:0.9615 test loss: 13.067455\n",
      "860: acrcuracy:0.98 loss: 6.5889063 (lr:0.001986476374697618)\n",
      "870: acrcuracy:0.96 loss: 14.228086 (lr:0.0019770675345263007)\n",
      "880: acrcuracy:0.96 loss: 7.752102 (lr:0.00196770562114111)\n",
      "890: acrcuracy:0.96 loss: 11.224881 (lr:0.0019583904004937245)\n",
      "900: acrcuracy:1.0 loss: 3.2642827 (lr:0.001949121639703143)\n",
      "900: ******* epoch 2 ******* test accuracy:0.9674 test loss: 10.376705\n",
      "910: acrcuracy:0.96 loss: 12.562491 (lr:0.001939899107049862)\n",
      "920: acrcuracy:0.97 loss: 7.3812747 (lr:0.0019307225719700854)\n",
      "930: acrcuracy:0.96 loss: 10.149274 (lr:0.0019215918050499584)\n",
      "940: acrcuracy:0.95 loss: 13.3625765 (lr:0.0019125065780198325)\n",
      "950: acrcuracy:0.97 loss: 17.654987 (lr:0.0019034666637485586)\n",
      "950: ******* epoch 2 ******* test accuracy:0.9682 test loss: 9.950099\n",
      "960: acrcuracy:0.96 loss: 7.9584255 (lr:0.0018944718362378086)\n",
      "970: acrcuracy:0.96 loss: 8.8041115 (lr:0.001885521870616427)\n",
      "980: acrcuracy:0.95 loss: 17.355726 (lr:0.0018766165431348069)\n",
      "990: acrcuracy:0.99 loss: 5.5649858 (lr:0.001867755631159297)\n",
      "1000: acrcuracy:1.0 loss: 1.3277636 (lr:0.0018589389131666372)\n",
      "1000: ******* epoch 2 ******* test accuracy:0.9665 test loss: 10.927924\n",
      "1010: acrcuracy:0.94 loss: 17.46791 (lr:0.0018501661687384176)\n",
      "1020: acrcuracy:0.98 loss: 7.6862187 (lr:0.0018414371785555712)\n",
      "1030: acrcuracy:0.98 loss: 10.1713295 (lr:0.0018327517243928889)\n",
      "1040: acrcuracy:0.97 loss: 8.673503 (lr:0.001824109589113564)\n",
      "1050: acrcuracy:0.93 loss: 15.417145 (lr:0.001815510556663764)\n",
      "1050: ******* epoch 2 ******* test accuracy:0.9672 test loss: 10.690953\n",
      "1060: acrcuracy:0.98 loss: 6.850043 (lr:0.0018069544120672303)\n",
      "1070: acrcuracy:0.94 loss: 14.732589 (lr:0.001798440941419902)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080: acrcuracy:0.95 loss: 15.3502035 (lr:0.0017899699318845701)\n",
      "1090: acrcuracy:0.98 loss: 7.9256983 (lr:0.0017815411716855546)\n",
      "1100: acrcuracy:0.97 loss: 9.083041 (lr:0.0017731544501034114)\n",
      "1100: ******* epoch 2 ******* test accuracy:0.9726 test loss: 8.981793\n",
      "1110: acrcuracy:0.95 loss: 16.643034 (lr:0.0017648095574696644)\n",
      "1120: acrcuracy:0.97 loss: 5.8758903 (lr:0.0017565062851615633)\n",
      "1130: acrcuracy:0.98 loss: 4.467147 (lr:0.0017482444255968678)\n",
      "1140: acrcuracy:0.95 loss: 14.83195 (lr:0.0017400237722286578)\n",
      "1150: acrcuracy:0.97 loss: 4.4703317 (lr:0.0017318441195401718)\n",
      "1150: ******* epoch 2 ******* test accuracy:0.9697 test loss: 9.5495615\n",
      "1160: acrcuracy:0.98 loss: 6.3855467 (lr:0.001723705263039666)\n",
      "1170: acrcuracy:0.97 loss: 7.406532 (lr:0.0017156069992553045)\n",
      "1180: acrcuracy:0.98 loss: 6.0264163 (lr:0.0017075491257300707)\n",
      "1190: acrcuracy:0.99 loss: 7.8419805 (lr:0.0016995314410167067)\n",
      "1200: acrcuracy:0.98 loss: 5.7251334 (lr:0.001691553744672677)\n",
      "1200: ******* epoch 3 ******* test accuracy:0.9726 test loss: 9.216861\n",
      "1210: acrcuracy:0.95 loss: 6.768439 (lr:0.0016836158372551574)\n",
      "1220: acrcuracy:0.97 loss: 11.372497 (lr:0.0016757175203160495)\n",
      "1230: acrcuracy:0.98 loss: 4.1754684 (lr:0.0016678585963970183)\n",
      "1240: acrcuracy:0.93 loss: 15.770668 (lr:0.001660038869024556)\n",
      "1250: acrcuracy:0.97 loss: 13.280517 (lr:0.001652258142705072)\n",
      "1250: ******* epoch 3 ******* test accuracy:0.9749 test loss: 8.507847\n",
      "1260: acrcuracy:0.99 loss: 2.804458 (lr:0.001644516222920002)\n",
      "1270: acrcuracy:0.99 loss: 6.785927 (lr:0.001636812916120949)\n",
      "1280: acrcuracy:0.99 loss: 4.038182 (lr:0.001629148029724841)\n",
      "1290: acrcuracy:0.99 loss: 3.3260207 (lr:0.0016215213721091197)\n",
      "1300: acrcuracy:0.97 loss: 7.2028494 (lr:0.0016139327526069466)\n",
      "1300: ******* epoch 3 ******* test accuracy:0.9745 test loss: 8.312696\n",
      "1310: acrcuracy:0.97 loss: 8.895148 (lr:0.00160638198150244)\n",
      "1320: acrcuracy:0.98 loss: 4.791547 (lr:0.0015988688700259279)\n",
      "1330: acrcuracy:0.99 loss: 3.7505765 (lr:0.0015913932303492327)\n",
      "1340: acrcuracy:0.98 loss: 8.422072 (lr:0.0015839548755809732)\n",
      "1350: acrcuracy:0.94 loss: 12.565301 (lr:0.0015765536197618927)\n",
      "1350: ******* epoch 3 ******* test accuracy:0.9719 test loss: 9.314588\n",
      "1360: acrcuracy:0.99 loss: 3.5929635 (lr:0.0015691892778602098)\n",
      "1370: acrcuracy:1.0 loss: 1.4817705 (lr:0.0015618616657669942)\n",
      "1380: acrcuracy:0.97 loss: 6.03054 (lr:0.0015545706002915614)\n",
      "1390: acrcuracy:0.98 loss: 4.775796 (lr:0.0015473158991568946)\n",
      "1400: acrcuracy:0.97 loss: 10.133984 (lr:0.0015400973809950877)\n",
      "1400: ******* epoch 3 ******* test accuracy:0.9746 test loss: 8.230165\n",
      "1410: acrcuracy:1.0 loss: 1.8418796 (lr:0.001532914865342811)\n",
      "1420: acrcuracy:1.0 loss: 2.1404116 (lr:0.001525768172636799)\n",
      "1430: acrcuracy:0.99 loss: 1.9601223 (lr:0.0015186571242093616)\n",
      "1440: acrcuracy:0.99 loss: 3.6548126 (lr:0.001511581542283918)\n",
      "1450: acrcuracy:0.97 loss: 6.1918654 (lr:0.0015045412499705513)\n",
      "1450: ******* epoch 3 ******* test accuracy:0.9716 test loss: 9.636123\n",
      "1460: acrcuracy:1.0 loss: 1.1181208 (lr:0.0014975360712615872)\n",
      "1470: acrcuracy:0.98 loss: 10.870606 (lr:0.0014905658310271931)\n",
      "1480: acrcuracy:1.0 loss: 1.6676826 (lr:0.0014836303550109999)\n",
      "1490: acrcuracy:0.97 loss: 8.43625 (lr:0.0014767294698257462)\n",
      "1500: acrcuracy:0.98 loss: 4.381852 (lr:0.0014698630029489428)\n",
      "1500: ******* epoch 3 ******* test accuracy:0.9716 test loss: 9.096052\n",
      "1510: acrcuracy:1.0 loss: 1.9724312 (lr:0.0014630307827185603)\n",
      "1520: acrcuracy:0.99 loss: 2.4340148 (lr:0.0014562326383287369)\n",
      "1530: acrcuracy:0.97 loss: 6.7850747 (lr:0.001449468399825509)\n",
      "1540: acrcuracy:0.99 loss: 8.647069 (lr:0.0014427378981025616)\n",
      "1550: acrcuracy:0.97 loss: 5.773543 (lr:0.001436040964897001)\n",
      "1550: ******* epoch 3 ******* test accuracy:0.9755 test loss: 7.971487\n",
      "1560: acrcuracy:0.98 loss: 8.277918 (lr:0.0014293774327851483)\n",
      "1570: acrcuracy:0.97 loss: 10.158321 (lr:0.0014227471351783538)\n",
      "1580: acrcuracy:1.0 loss: 1.4799403 (lr:0.001416149906318832)\n",
      "1590: acrcuracy:0.97 loss: 14.04958 (lr:0.0014095855812755176)\n",
      "1600: acrcuracy:0.98 loss: 4.232566 (lr:0.0014030539959399427)\n",
      "1600: ******* epoch 3 ******* test accuracy:0.9716 test loss: 9.603687\n",
      "1610: acrcuracy:0.99 loss: 2.9655197 (lr:0.0013965549870221337)\n",
      "1620: acrcuracy:0.99 loss: 1.339634 (lr:0.0013900883920465294)\n",
      "1630: acrcuracy:0.97 loss: 10.621797 (lr:0.0013836540493479186)\n",
      "1640: acrcuracy:1.0 loss: 2.017354 (lr:0.001377251798067398)\n",
      "1650: acrcuracy:0.99 loss: 3.0696967 (lr:0.001370881478148353)\n",
      "1650: ******* epoch 3 ******* test accuracy:0.9746 test loss: 8.488176\n",
      "1660: acrcuracy:0.98 loss: 8.623098 (lr:0.0013645429303324535)\n",
      "1670: acrcuracy:1.0 loss: 2.5605166 (lr:0.0013582359961556737)\n",
      "1680: acrcuracy:1.0 loss: 2.3318777 (lr:0.0013519605179443314)\n",
      "1690: acrcuracy:1.0 loss: 0.94116217 (lr:0.0013457163388111437)\n",
      "1700: acrcuracy:0.97 loss: 8.997287 (lr:0.0013395033026513076)\n",
      "1700: ******* epoch 3 ******* test accuracy:0.976 test loss: 8.292198\n",
      "1710: acrcuracy:0.98 loss: 5.2242684 (lr:0.001333321254138595)\n",
      "1720: acrcuracy:0.96 loss: 5.687646 (lr:0.0013271700387214717)\n",
      "1730: acrcuracy:0.99 loss: 2.378947 (lr:0.0013210495026192315)\n",
      "1740: acrcuracy:0.97 loss: 10.025913 (lr:0.0013149594928181531)\n",
      "1750: acrcuracy:0.99 loss: 2.4359648 (lr:0.0013088998570676745)\n",
      "1750: ******* epoch 3 ******* test accuracy:0.9759 test loss: 8.228381\n",
      "1760: acrcuracy:0.98 loss: 7.483652 (lr:0.0013028704438765861)\n",
      "1770: acrcuracy:0.99 loss: 3.7713044 (lr:0.0012968711025092442)\n",
      "1780: acrcuracy:0.98 loss: 5.113217 (lr:0.001290901682981802)\n",
      "1790: acrcuracy:0.95 loss: 6.584419 (lr:0.0012849620360584606)\n",
      "1800: acrcuracy:0.99 loss: 2.0609543 (lr:0.0012790520132477375)\n",
      "1800: ******* epoch 4 ******* test accuracy:0.9764 test loss: 7.670228\n",
      "1810: acrcuracy:0.99 loss: 3.2749403 (lr:0.0012731714667987546)\n",
      "1820: acrcuracy:1.0 loss: 0.56557304 (lr:0.0012673202496975445)\n",
      "1830: acrcuracy:0.99 loss: 1.9489324 (lr:0.0012614982156633745)\n",
      "1840: acrcuracy:0.99 loss: 2.0948696 (lr:0.0012557052191450912)\n",
      "1850: acrcuracy:1.0 loss: 2.1063466 (lr:0.0012499411153174794)\n",
      "1850: ******* epoch 4 ******* test accuracy:0.9774 test loss: 7.3310432\n",
      "1860: acrcuracy:1.0 loss: 2.775419 (lr:0.0012442057600776434)\n",
      "1870: acrcuracy:0.99 loss: 1.6712686 (lr:0.0012384990100414034)\n",
      "1880: acrcuracy:1.0 loss: 2.0627065 (lr:0.0012328207225397114)\n",
      "1890: acrcuracy:0.99 loss: 9.692763 (lr:0.001227170755615084)\n",
      "1900: acrcuracy:0.97 loss: 5.6244106 (lr:0.0012215489680180538)\n",
      "1900: ******* epoch 4 ******* test accuracy:0.9749 test loss: 8.600649\n",
      "1910: acrcuracy:0.99 loss: 4.1577516 (lr:0.001215955219203638)\n",
      "1920: acrcuracy:1.0 loss: 0.9117077 (lr:0.0012103893693278251)\n",
      "1930: acrcuracy:0.99 loss: 4.132959 (lr:0.0012048512792440782)\n",
      "1940: acrcuracy:0.99 loss: 2.1758773 (lr:0.0011993408104998568)\n",
      "1950: acrcuracy:0.97 loss: 9.7192955 (lr:0.0011938578253331553)\n",
      "1950: ******* epoch 4 ******* test accuracy:0.9789 test loss: 7.469952\n",
      "1960: acrcuracy:1.0 loss: 0.7926078 (lr:0.001188402186669059)\n",
      "1970: acrcuracy:1.0 loss: 1.6495988 (lr:0.0011829737581163168)\n",
      "1980: acrcuracy:0.98 loss: 6.4802833 (lr:0.0011775724039639326)\n",
      "1990: acrcuracy:0.99 loss: 1.454785 (lr:0.0011721979891777712)\n",
      "2000: acrcuracy:0.99 loss: 6.276936 (lr:0.0011668503793971828)\n",
      "2000: ******* epoch 4 ******* test accuracy:0.9755 test loss: 7.9456315\n",
      "max test accuracy: 0.9789\n"
     ]
    }
   ],
   "source": [
    "max_accuracy = 0.0\n",
    "\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "    \n",
    "    global max_accuracy\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    \n",
    "    max_learning_rate = 0.003\n",
    "    min_learning_rate = 0.0001\n",
    "    decay_speed = 2000.0\n",
    "    learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) *math.exp(-i/decay_speed)\n",
    "    \n",
    "    if(update_train_data):\n",
    "        a, c = sess.run([accuracy, cross_entropy], feed_dict = {X:batch_X, Y_:batch_Y, pkeep: 1.0})\n",
    "        print(str(i) + \": acrcuracy:\" + str(a) + \" loss: \" + str(c) + \" (lr:\" + str(learning_rate) + \")\")\n",
    "    \n",
    "    if(update_test_data):\n",
    "        a, c = sess.run([accuracy, cross_entropy], feed_dict = {X:mnist.test.images, Y_:mnist.test.labels, pkeep: 1.0})\n",
    "        if(a>max_accuracy):\n",
    "            max_accuracy = a\n",
    "        print(str(i) + \": ******* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ******* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "    sess.run (train_step, feed_dict={X: batch_X, Y_: batch_Y, pkeep: 0.75, lr: learning_rate})\n",
    "\n",
    "for i in range(2000+1):\n",
    "    training_step(i, i%50==0, i%10 == 0)\n",
    "print(\"max test accuracy: \" + str(max_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.8]",
   "language": "python",
   "name": "conda-env-tf1.8-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
